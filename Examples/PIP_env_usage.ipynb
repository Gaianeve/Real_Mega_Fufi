{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leOlH6jswZFC",
        "outputId": "5011106c-b504-45f0-bd8f-3e530cc032fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (3.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (0.0.8)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting pybullet\n",
            "  Using cached pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Using cached pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n",
            "Installing collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.6\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.10/dist-packages (3.1.43)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython) (4.0.11)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython) (5.0.1)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install gym==0.25.2\n",
        "#needed from March\n",
        "!pip install numpy==1.23.5\n",
        "!pip install pybullet\n",
        "!pip install GitPython\n",
        "\n",
        "!pip install stable_baselines3 -qU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import random\n",
        "import time\n",
        "from distutils.util import strtobool\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nde97PiPTFqw",
        "outputId": "fce294db-7a8b-4938-94c2-1dfdf4e7d3d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Gaianeve/gym-PIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgzID1XjxY1k",
        "outputId": "70123f0c-eee3-4d0f-fdef-08892d63e983"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gym-PIP'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 86 (delta 12), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (86/86), 25.07 KiB | 1.93 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install /content/gym-PIP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw_zo1S5xbHn",
        "outputId": "876612be-da73-44ec-96e9-6fbe61e9c5bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./gym-PIP\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter the environment directory\n",
        "%cd /content/gym-PIP\n",
        "\n",
        "import gym\n",
        "# Importing environment\n",
        "import gym_PIP\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu0r4BCWxkBI",
        "outputId": "6db25f16-c20d-43aa-c34f-3db58d6b4a52"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gym-PIP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Optional, Union\n",
        "\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class HistoryWrapper(gym.Wrapper):\n",
        "    \"\"\"Track history of observations for a given number of steps. Initial steps are zero-filled.\"\"\"\n",
        "\n",
        "    def __init__(self, env, steps, beta, use_continuity_cost):\n",
        "        super().__init__(env)\n",
        "        assert steps > 1, \"steps must be > 1\"\n",
        "        self.steps = steps\n",
        "        self.use_continuity_cost = use_continuity_cost\n",
        "        self.beta = beta  # weight of continuity cost\n",
        "\n",
        "        # Initialize step_low and step_high\n",
        "        self.step_low = np.concatenate([self.observation_space.low, self.action_space.low])\n",
        "        self.step_high = np.concatenate([self.observation_space.high, self.action_space.high])\n",
        "\n",
        "        # Stack for each step\n",
        "        obs_low = np.tile(self.step_low, (self.steps, 1))\n",
        "        obs_high = np.tile(self.step_high, (self.steps, 1))\n",
        "\n",
        "        self.observation_space = Box(low=obs_low.flatten(), high=obs_high.flatten())\n",
        "\n",
        "        self.history = self._make_history()\n",
        "\n",
        "    def _make_history(self):\n",
        "        return [np.zeros_like(self.step_low) for _ in range(self.steps)]\n",
        "\n",
        "    def _continuity_cost(self, obs):\n",
        "        action = obs[-1][-self.action_space.shape[0]:]\n",
        "        last_action = obs[-2][-self.action_space.shape[0]:]\n",
        "        continuity_cost = np.power((action - last_action), 2).sum()\n",
        "        return continuity_cost\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "\n",
        "        #check obs are in the right form for concatenation\n",
        "        if obs.shape != (12,):\n",
        "          obs = obs.ravel()\n",
        "\n",
        "        self.history.pop(0)\n",
        "\n",
        "        obs = np.concatenate([obs, action])\n",
        "        self.history.append(obs)\n",
        "        obs = np.array(self.history, dtype=np.float32)\n",
        "\n",
        "        if self.use_continuity_cost:\n",
        "            continuity_cost = self._continuity_cost(obs)\n",
        "            reward -= self.beta * continuity_cost\n",
        "            info[\"continuity_cost\"] = continuity_cost\n",
        "\n",
        "        return obs.flatten(), reward, done, info\n",
        "\n",
        "    def reset(\n",
        "    self,\n",
        "    seed: Optional[int] = None,\n",
        "    options: Optional[dict] = None,\n",
        "    ):\n",
        "      # Chiama il reset dell'ambiente per ottenere l'osservazione iniziale\n",
        "      obs = self.env.reset(seed=seed, options=options)\n",
        "\n",
        "      # Assicurati che obs abbia la forma corretta (aggiungi zeri per le azioni)\n",
        "      obs = np.concatenate([\n",
        "          obs,\n",
        "          np.zeros_like(self.env.action_space.low)\n",
        "      ])\n",
        "\n",
        "      # Imposta self.history con la forma corretta\n",
        "      self.history = [np.zeros_like(obs) for _ in range(self.steps)]\n",
        "      self.history[0] = obs  # Imposta la prima osservazione\n",
        "\n",
        "      # Restituisce la storia come array appiattito\n",
        "      return np.array(self.history, dtype=np.float32).flatten()"
      ],
      "metadata": {
        "id": "G4LSb8gIMspr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# making the environment\n",
        "def make_env(gym_id, seed, idx, capture_video, run_name, beta):\n",
        "    def thunk():\n",
        "        print('Dissennatore')\n",
        "        env = gym.make(gym_id)\n",
        "        print('done environment')\n",
        "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "        env = HistoryWrapper(env, 4, beta, True)\n",
        "         # Add this check after creating the env instance\n",
        "        if not hasattr(env, 'pippa_id'):  # Or another suitable check for successful model loading\n",
        "            raise ValueError(\"Robot model not properly loaded in the environment.\")\n",
        "        if capture_video:\n",
        "            if idx == 0:\n",
        "              #record video every 500 episodes\n",
        "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\", \\\n",
        "                                               episode_trigger = lambda x: x % 500 == 0)\n",
        "        env.seed(seed)\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "        return env\n",
        "\n",
        "    return thunk\n",
        "\n",
        "\"\"\"## Create parallel environments  ðŸŒ ðŸ¦\"\"\"\n",
        "\n",
        "# vectorize environment\n",
        "def vectorize_env(gym_id, seed, capture_video, run_name, num_envs, beta):\n",
        "  envs = gym.vector.SyncVectorEnv(\n",
        "        [make_env(gym_id, seed + i, i, capture_video, run_name, beta) for i in range(num_envs)]\n",
        "  )\n",
        "  assert isinstance(envs.single_action_space, gym.spaces.Box), \\\n",
        "  \"only continuous action space as Box is supported\"\n",
        "  return envs"
      ],
      "metadata": {
        "id": "Yo5tM2fXxx38"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"gSDE_class.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/14-PmUmqjlT_F3frLO4-ootagzOeJ1EZS\n",
        "\"\"\"\n",
        "\n",
        "from typing import Any, Dict, List, Optional, Tuple, TypeVar, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from gym import spaces\n",
        "from torch import nn\n",
        "\n",
        "## Needed class for gSDE ðŸŽ¡\n",
        "\"\"\"\n",
        "The **TanhBijector** class defines a bijective transformation using the hyperbolic tangent function (tanh). This class is often used in reinforcement learning algorithms to squash the output of the policy network to ensure that the actions remain within a specific range, typically [-1,1]\n",
        "\"\"\"\n",
        "\n",
        "from stable_baselines3.common.distributions import Distribution, TanhBijector, sum_independent_dims\n",
        "from torch.distributions import Bernoulli, Categorical, Normal\n",
        "\n",
        "\"\"\"# State Dependent Noise Distribution (gSDE) ðŸŒµ\n",
        "ðŸª„ Distribution class for using generalized State Dependent Exploration (gSDE).\n",
        "\n",
        "Paper: https://arxiv.org/abs/2005.05719 ðŸ¦„\n",
        "\n",
        "It is used to create the noise exploration matrix and compute the log probability of an action with that noise.\n",
        "\n",
        "   * :`param action_dim`: Dimension of the action space.\n",
        "   * :`param full_std:` Whether to use (n_features x n_actions) parameters for the std instead of only (n_features,)\n",
        "   * :`param use_expln:` Use `expln()` function instead of `exp()` to ensure a positive standard deviation (cf paper). It allows to keep variance above zero and prevent it from growing too fast. In practice, `exp()` is usually enough.\n",
        "   * `:param squash_output`: Whether to squash the output using a tanh function, this ensures bounds are satisfied.\n",
        "\n",
        "   * `:param learn_features`: Whether to learn features for gSDE or not. This will enable gradients to be backpropagated through the features ``latent_sde`` in the code.\n",
        "\n",
        "   * `:param epsilon:` small value to avoid NaN due to numerical imprecision.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"## gSDE class ðŸ§\"\"\"\n",
        "\n",
        "class gSDE(Distribution):\n",
        "    bijector: Optional[TanhBijector]\n",
        "    weights_dist: Normal\n",
        "    exploration_mat: th.Tensor\n",
        "    exploration_matrices: th.Tensor\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        action_dim: int,\n",
        "        observation_dim: int,\n",
        "        observation: th.Tensor,\n",
        "        mean_actions: th.Tensor,\n",
        "        log_std: th.Tensor,\n",
        "        full_std: bool = True,\n",
        "        use_expln: bool = False,\n",
        "        squash_output: bool = False,\n",
        "        learn_features: bool = True,\n",
        "        epsilon: float = 1e-6,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.action_dim = action_dim\n",
        "        self.obs_dim = observation_dim\n",
        "        self.x = observation\n",
        "        self.mean_actions = mean_actions\n",
        "        self.log_std = log_std\n",
        "        self.use_expln = use_expln\n",
        "        self.full_std = full_std\n",
        "        self.epsilon = epsilon\n",
        "        self.learn_features = learn_features\n",
        "        self.bijector = TanhBijector(epsilon) if squash_output else None\n",
        "\n",
        "        #get combination of action and coordinates for noise computation\n",
        "        # Linear layer to output flattened dimensions\n",
        "        self.latent_sde = th.nn.Linear(self.obs_dim,1)\n",
        "        self._latent_sde = self.latent_sde(self.x)\n",
        "        self.latent_sde_dim = self._latent_sde.shape\n",
        "\n",
        "        #initializing distribution of actions\n",
        "        self.distribution = self.proba_distribution()\n",
        "\n",
        "    #-------------------------------------------- get actions ------------------------------------------------------\n",
        "    def get_std(self) -> th.Tensor:\n",
        "        \"\"\"\n",
        "        Get the standard deviation from the learned parameter\n",
        "        (log of it by default). This ensures that the std is positive.\n",
        "\n",
        "        :param log_std:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if self.use_expln:\n",
        "            # From gSDE paper, it allows to keep variance\n",
        "            # above zero and prevent it from growing too fast\n",
        "            below_threshold = th.exp(self.log_std) * (self.log_std <= 0)\n",
        "            # Avoid NaN: zeros values that are below zero\n",
        "            self.safe_log_std = self.log_std * (self.log_std > 0) + self.epsilon\n",
        "            above_threshold = (th.log1p(self.safe_log_std) + 1.0) * (self.log_std > 0)\n",
        "            std = below_threshold + above_threshold\n",
        "        else:\n",
        "            # Use normal exponential\n",
        "            std = th.exp(self.log_std)\n",
        "\n",
        "        if self.full_std:\n",
        "            return std\n",
        "        assert self.latent_sde_dim is not None\n",
        "        # Reduce the number of parameters:\n",
        "        return th.ones(self.latent_sde_dim, self.action_dim).to(self.log_std.device) * std\n",
        "\n",
        "\n",
        "\n",
        "    def sample_weights(self, batch_size: int = 1)-> th.Tensor:\n",
        "      \"\"\"\n",
        "      Sample weights for the noise exploration matrix,\n",
        "      using a centered Gaussian distribution.\n",
        "\n",
        "      :param log_std:\n",
        "      :param batch_size:\n",
        "      \"\"\"\n",
        "      std = self.get_std()\n",
        "      self.weights_dist = Normal(th.zeros_like(std), std)\n",
        "      # Reparametrization trick to pass gradients\n",
        "      self.exploration_mat = self.weights_dist.rsample()\n",
        "      # Pre-compute matrices in case of parallel exploration\n",
        "      exploration_matrices = self.weights_dist.rsample((batch_size,))\n",
        "      return exploration_matrices\n",
        "\n",
        "    def get_noise(self, batch_size: int = 1) -> th.Tensor:\n",
        "        self.exploration_matrices = self.sample_weights()\n",
        "        self._latent_sde = self._latent_sde if self.learn_features else self._latent_sde.detach()\n",
        "        if len(self._latent_sde) == 1 or len(self._latent_sde) != len(self.exploration_mat):\n",
        "            return self._latent_sde * self.exploration_mat\n",
        "        noise = self._latent_sde * self.exploration_mat\n",
        "        return noise\n",
        "\n",
        "\n",
        "    def proba_distribution(self) -> Normal:\n",
        "        \"\"\"\n",
        "        Create the distribution given its parameters (mean, std)\n",
        "\n",
        "        :param mean_actions:\n",
        "        :param log_std:\n",
        "        :param latent_sde:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Stop gradient if we don't want to influence the features\n",
        "        self._latent_sde = self._latent_sde if self.learn_features else self._latent_sde.detach()\n",
        "        variance = self._latent_sde**2*self.get_std() ** 2\n",
        "        distribution = Normal(self.mean_actions, th.sqrt(variance + self.epsilon))\n",
        "        return distribution\n",
        "\n",
        "    #get action\n",
        "    def sample(self) -> th.Tensor:\n",
        "        noise = self.get_noise()\n",
        "        self.distribution = self.proba_distribution()\n",
        "        actions = self.distribution.mean + noise\n",
        "        if self.bijector is not None:\n",
        "            return self.bijector.forward(actions)\n",
        "        return actions\n",
        "\n",
        "# --------------------------------------------- logprobs and entropy --------------------------------------------\n",
        "#to be both called after self.sample, otherwise self.distribution wuold be Nan and you get nothing\n",
        "\n",
        "    def log_prob(self, actions: th.Tensor) -> th.Tensor:\n",
        "        if self.bijector is not None:\n",
        "            gaussian_actions = self.bijector.inverse(actions)\n",
        "        else:\n",
        "            gaussian_actions = actions\n",
        "        # log likelihood for a gaussian\n",
        "        log_prob = self.distribution.log_prob(gaussian_actions)\n",
        "        # Sum along action dim\n",
        "        log_prob = sum_independent_dims(log_prob)\n",
        "\n",
        "        if self.bijector is not None:\n",
        "            # Squash correction (from original SAC implementation)\n",
        "            log_prob -= th.sum(self.bijector.log_prob_correction(gaussian_actions), dim=1)\n",
        "        return log_prob\n",
        "\n",
        "    def entropy(self) -> Optional[th.Tensor]:\n",
        "        if self.bijector is not None:\n",
        "            # No analytical form,\n",
        "            # entropy needs to be estimated using -log_prob.mean()\n",
        "            return None\n",
        "        return sum_independent_dims(self.distribution.entropy())\n",
        "\n",
        "\n",
        "# ---------------------------------------------- auxiliary functions  ------------------------------------------------\n",
        "#not stricly needed for my PPO. Might still be useful, so keep them\n",
        "    def mode(self) -> th.Tensor:\n",
        "        actions = self.distribution.mean\n",
        "        if self.bijector is not None:\n",
        "            return self.bijector.forward(actions)\n",
        "        return actions\n",
        "\n",
        "    def actions_from_params(\n",
        "        self, mean_actions: th.Tensor, deterministic: bool = False\n",
        "    ) -> th.Tensor:\n",
        "        # Update the proba distribution\n",
        "        self.proba_distribution(mean_actions, self.log_std)\n",
        "        return self.get_actions(deterministic=deterministic)\n",
        "\n",
        "    def log_prob_from_params(\n",
        "        self, mean_actions: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
        "        actions = self.actions_from_params(mean_actions)\n",
        "        log_prob = self.log_prob(actions)\n",
        "        return actions, log_prob\n",
        "\n",
        "    def proba_distribution_net(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "VLPMDnBXT0pv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Agent_class__gSDE.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/github/Gaianeve/The_real_Fufi/blob/main/Software/Agent_class__gSDE.ipynb\n",
        "\n",
        "# Agent class ðŸ¤–\n",
        "Defining the the actor-critic NN structure.\n",
        "\"\"\"\n",
        "\n",
        "# importing libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torchsummary import summary\n",
        "import gym\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "#getting cute unique name for checkpoint\n",
        "def get_checkpoint_name(epoch_v):\n",
        "  now = datetime.now()\n",
        "  today = now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
        "  check_name = 'checkpoint' + '_' + str(epoch_v) + '_' + today\n",
        "  return check_name\n",
        "\n",
        "\"\"\"## PPO structure ðŸ¦„ âœ¨\n",
        "Defining the basic layer for PPO\n",
        "\"\"\"\n",
        "\n",
        "# init layer\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "  torch.nn.init.orthogonal_(layer.weight, std)\n",
        "  torch.nn.init.constant_(layer.bias, bias_const)\n",
        "  return layer\n",
        "\n",
        "\"\"\"## Here's the actual agent ðŸ¶ ðŸ¦¾\n",
        "ðŸª„ Differences from the previous versions:\n",
        "* Added gSDE:\n",
        "Mostly taken from stablebaseline implementation [here](https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/distributions.py), called\n",
        "```\n",
        "StateDependentNoiseDistribution\n",
        "```\n",
        "Paper [here](https://arxiv.org/abs/2005.05719)\n",
        "\n",
        "* PPO for a continous action space. The code is that of the CleanRL implementation [here](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_continuous_action.py).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# agent class\n",
        "class Agent(nn.Module):\n",
        "  def __init__(self, envs, use_sde):\n",
        "      super().__init__()\n",
        "      #assign environment to interact with\n",
        "      self.envs = envs\n",
        "      #gSDE flag\n",
        "      self.use_sde = use_sde\n",
        "\n",
        "      #actor critic NN\n",
        "      self.critic = nn.Sequential(\n",
        "          layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
        "          nn.Tanh(),\n",
        "          layer_init(nn.Linear(64, 64)),\n",
        "          nn.Tanh(),\n",
        "          layer_init(nn.Linear(64, 1), std=1.0),\n",
        "      )\n",
        "      self.actor_mean = nn.Sequential(\n",
        "          layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
        "          nn.Tanh(),\n",
        "          layer_init(nn.Linear(64, 64)),\n",
        "          nn.Tanh(),\n",
        "          layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),\n",
        "      )\n",
        "      self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))\n",
        "      #learn log of standard dev\n",
        "\n",
        "  ## keep in mind that x are the observations\n",
        "  def get_value(self, x):\n",
        "      return self.critic(x)\n",
        "\n",
        "  def get_action_and_value(self, x, action=None):\n",
        "      action_mean = self.actor_mean(x)\n",
        "      action_logstd = self.actor_logstd.expand_as(action_mean) #match dimention of action mean\n",
        "\n",
        "      if self.use_sde:\n",
        "        #sample from SDE distribution\n",
        "        action_dim = np.prod(self.envs.single_action_space.shape)\n",
        "        int_latent = np.array(self.envs.single_observation_space.shape).prod()\n",
        "        probs = gSDE(action_dim = action_dim, observation_dim = int_latent, observation = x, mean_actions = action_mean, log_std = action_logstd)\n",
        "      else:\n",
        "        #sample from standard gaussian\n",
        "        action_std = torch.exp(action_logstd)\n",
        "        probs = Normal(action_mean, action_std)\n",
        "\n",
        "      if action is None:\n",
        "          action = probs.sample()\n",
        "      return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
        "\n",
        "    # NN summary\n",
        "  def print_summary(self, envs):\n",
        "    print('Actor summary')\n",
        "    print(summary(self.actor, envs.single_observation_space.shape))\n",
        "    print('Critic summary')\n",
        "    print(summary(self.critic, envs.single_observation_space.shape))\n",
        "\n",
        "  def get_parameters(self):\n",
        "    #useful if wanting to check the updating of NN parameters\n",
        "    for name, param in self.named_parameters():\n",
        "      print(name, param.data)\n",
        "\n",
        "   #checkpoints\n",
        "  def save_checkpoint(self, epoch_v):\n",
        "      checkpoint_name = get_checkpoint_name(epoch_v)\n",
        "      directory = os.getcwd() + '/' + 'checkpoints/'\n",
        "      #if it doesn't exists, then create it\n",
        "      if not os.path.exists(directory):\n",
        "          os.mkdir(directory)\n",
        "          print('Dear human, checkpoint directory did not existed. I created it for you ')\n",
        "      path = directory + checkpoint_name\n",
        "      print(\"=> saving checkpoint '{}'\".format(path))\n",
        "      torch.save(self.state_dict(), path)\n",
        "\n",
        "  def resume_from_checkpoint(self, path):\n",
        "      print(\"=> loading checkpoint '{}'\".format(path))\n",
        "      self.load_state_dict(torch.load(path))\n",
        "\n",
        "  def save_agent(self, file_name):\n",
        "      directory = os.getcwd() + '/' + 'models/'\n",
        "      #if it doesn't exists, then create it\n",
        "      if not os.path.exists(directory):\n",
        "          os.mkdir(directory)\n",
        "          print('Dear human, saved model directory did not existed. I created it for you ')\n",
        "      path = directory + file_name\n",
        "      print(\"=> saving model as best agent in '{}'\".format(path))\n",
        "      torch.save(self.state_dict(), path)\n",
        "\n",
        "  def load_agent(self, path):\n",
        "      print(\"=> loading model from '{}'\".format(path))\n",
        "      self.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDQ4r_37T-qW",
        "outputId": "b506356c-b293-48f9-aef1-2a6c19aa44ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Agent_utils.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/github/Gaianeve/FUFONE/blob/main/PPO/Agent_utils.ipynb\n",
        "\n",
        "# Agent related functions.\n",
        "\"\"\"\n",
        "\n",
        "#libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "\"\"\"## Annealing\n",
        "Takes care of annealing, AKA decreasing the learning rate, if requested\n",
        "\"\"\"\n",
        "\n",
        "#exponentially descrease the learning rate if toggled\n",
        "def anneal(anneal_lr, update_step, num_update, learning_rate):\n",
        "  if anneal_lr:\n",
        "    frac = 1.0 - (update_step - 1.0) / num_update\n",
        "    lrnow = frac * learning_rate\n",
        "    return lrnow\n",
        "  else:\n",
        "    return learning_rate\n",
        "\n",
        "\"\"\"## Updating agent\n",
        "Takes care of updating agent relating lists\n",
        "\"\"\"\n",
        "\n",
        "def collect_data(envs_v, obs_v, actions_v, logprobs_v, rewards_v,\\\n",
        "           dones_v, values_v, next_obs_v, next_done_v, agent_v,\\\n",
        "                 step_loop, device_v):\n",
        "  obs_v[step_loop] = next_obs_v\n",
        "  dones_v[step_loop] = next_done_v\n",
        "\n",
        "  # ALGO LOGIC: action logic\n",
        "  with torch.no_grad():\n",
        "      action, logprob, _, value = agent_v.get_action_and_value(next_obs_v)\n",
        "      values_v[step_loop] = value.flatten()\n",
        "  actions_v[step_loop] = action\n",
        "  logprobs_v[step_loop] = logprob\n",
        "\n",
        "  # TRY NOT TO MODIFY: execute the game and log data.\n",
        "  next_obs_v, reward, done, info_v = envs_v.step(action.cpu().numpy())\n",
        "  rewards_v[step_loop] = torch.tensor(reward).to(device_v).view(-1)\n",
        "  next_obs_v, next_done_v = torch.Tensor(next_obs_v).to(device_v), torch.Tensor(done).to(device_v)\n",
        "\n",
        "  return obs_v, actions_v, logprobs_v, rewards_v, dones_v, values_v, next_obs_v, next_done_v, info_v\n",
        "\n",
        "\"\"\"## GAE\n",
        "General Advantage estimation, basically an algorithm to estimate the advantage function.\n",
        "\n",
        "The original paper about GAE can be found [here](https://arxiv.org/abs/1506.02438)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def GAE(gae_v, gae_lambda_v, gamma_v, agent_v,\\\n",
        "        values_v, dones_v, rewards_v, next_obs_v, next_done_v,\\\n",
        "        num_steps_v, device_v):\n",
        "  # bootstrap value if not done\n",
        "  with torch.no_grad():\n",
        "    next_value = agent_v.get_value(next_obs_v).reshape(1, -1)\n",
        "    if gae_v:\n",
        "        advantages_v = torch.zeros_like(rewards_v).to(device_v)\n",
        "        lastgaelam = 0\n",
        "        for t in reversed(range(num_steps_v)):\n",
        "            if t == num_steps_v - 1:\n",
        "                nextnonterminal = 1.0 - next_done_v\n",
        "                nextvalues = next_value\n",
        "            else:\n",
        "                nextnonterminal = 1.0 - dones_v[t + 1]\n",
        "                nextvalues = values_v[t + 1]\n",
        "            delta = rewards_v[t] + gamma_v * nextvalues * nextnonterminal - values_v[t]\n",
        "            advantages_v[t] = lastgaelam =\\\n",
        "             delta + gamma_v * gae_lambda_v * nextnonterminal * lastgaelam\n",
        "        returns_v = advantages_v + values_v\n",
        "    else:\n",
        "        returns_v = torch.zeros_like(rewards_v).to(device_v)\n",
        "        for t in reversed(range(num_steps_v)):\n",
        "            if t == num_steps_v - 1:\n",
        "                nextnonterminal = 1.0 - next_done_v\n",
        "                next_return = next_value\n",
        "            else:\n",
        "                nextnonterminal = 1.0 - dones_v[t + 1]\n",
        "                next_return = returns[t + 1]\n",
        "            returns_v[t] = rewards_v[t] + gamma_v * nextnonterminal * next_return\n",
        "        advantages_v = returns_v - values_v\n",
        "\n",
        "  return returns_v, advantages_v\n",
        "\n",
        "\"\"\"## PPO training loop\n",
        "Training loop with PPO algorithm.\n",
        "\n",
        "We added kl divergence, that, in a nutshell, it's a simple way to understand how aggressive the policy updates. Further details about the calculation can be found [here](http://joschu.net/blog/kl-approx.html).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def PPO_train_agent(batch_size, update_epochs, minibatch_size, clip_coef, norm_adv, clip_vloss,\\\n",
        "                ent_coef, vf_coef, max_grad_norm, target_kl, \\\n",
        "                agent_v, optimizer_v,scheduler_v, scheduler_flag,\\\n",
        "                b_obs_v, b_actions_v,b_logprobs_v, b_advantages_v, b_returns_v, b_values_v,\\\n",
        "                checkpoint = False):\n",
        "\n",
        "  #checkpoint is a bool that decides whether to enable or not checkpoint saving\n",
        "\n",
        "  # Optimizing the policy and value network\n",
        "  b_inds = np.arange(batch_size)\n",
        "  clipfracs = []\n",
        "  loss = 0\n",
        "\n",
        "  for epoch in range(update_epochs):\n",
        "      #print('Starting epoch {} of training'.format(epoch))\n",
        "      np.random.shuffle(b_inds)\n",
        "      #calculate ratio\n",
        "      for start in range(0, batch_size, minibatch_size):\n",
        "          end = start + minibatch_size\n",
        "          mb_inds = b_inds[start:end]\n",
        "\n",
        "          _, newlogprob, entropy, newvalue = agent_v.get_action_and_value(b_obs_v[mb_inds],\\\n",
        "                                                                          b_actions_v.long()[mb_inds])\n",
        "          logratio = newlogprob - b_logprobs_v[mb_inds]\n",
        "          ratio = logratio.exp()\n",
        "\n",
        "          # calculate approx_kl\n",
        "          with torch.no_grad():\n",
        "              old_approx_kl = (-logratio).mean()\n",
        "              approx_kl = ((ratio - 1) - logratio).mean()\n",
        "              clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
        "\n",
        "          mb_advantages = b_advantages_v[mb_inds]\n",
        "          if norm_adv:\n",
        "              mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
        "\n",
        "          # Policy loss\n",
        "          pg_loss1 = -mb_advantages * ratio\n",
        "          pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
        "          pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "          # Value loss\n",
        "          newvalue = newvalue.view(-1)\n",
        "          if clip_vloss:\n",
        "              v_loss_unclipped = (newvalue - b_returns_v[mb_inds]) ** 2\n",
        "              v_clipped = b_values_v[mb_inds] + torch.clamp(\n",
        "                  newvalue - b_values_v[mb_inds],\n",
        "                  -clip_coef,\n",
        "                  clip_coef,\n",
        "              )\n",
        "              v_loss_clipped = (v_clipped - b_returns_v[mb_inds]) ** 2\n",
        "              v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
        "              v_loss = 0.5 * v_loss_max.mean()\n",
        "          else:\n",
        "              v_loss = 0.5 * ((newvalue - b_returns_v[mb_inds]) ** 2).mean()\n",
        "\n",
        "          entropy_loss = entropy.mean()\n",
        "          loss_value = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
        "\n",
        "          #get checkpoints before updating\n",
        "          if checkpoint:\n",
        "            if loss_value < loss:\n",
        "              agent_v.checkpoint(epoch)\n",
        "\n",
        "          loss = loss_value\n",
        "          optimizer_v.zero_grad()\n",
        "          loss.backward()\n",
        "          nn.utils.clip_grad_norm_(agent_v.parameters(), max_grad_norm)\n",
        "          optimizer_v.step()\n",
        "          if scheduler_flag:\n",
        "            scheduler_v.step(loss_value)\n",
        "\n",
        "      if target_kl is not None:\n",
        "          if approx_kl > target_kl:\n",
        "              break\n",
        "\n",
        "\n",
        "  return v_loss, pg_loss, entropy_loss, old_approx_kl, approx_kl, clipfracs, b_values_v, b_returns_v\n",
        "\n",
        "\"\"\"## Evaluate the agent\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def evaluate_agent(agent_v,gym_id, seed, device, beta,\\\n",
        "                   num_episodes = 10, step_evaluation = 500, eval_with_video = True):\n",
        "\n",
        "  # make a brand new environment and record the video\n",
        "  evaluation_video = f'video_evaluation'\n",
        "  env = vectorize_env(gym_id, seed, eval_with_video, evaluation_video, 1, beta)\n",
        "\n",
        "  #initialize storage lists\n",
        "  obs = torch.zeros((step_evaluation, 1) + env.observation_space.shape).to(device)\n",
        "  actions = torch.zeros((step_evaluation, 1) + env.action_space.shape).to(device)\n",
        "  logprobs = torch.zeros((step_evaluation, 1)).to(device)\n",
        "  rewards = torch.zeros((step_evaluation, 1)).to(device)\n",
        "  dones = torch.zeros((step_evaluation, 1)).to(device)\n",
        "  values = torch.zeros((step_evaluation, 1)).to(device)\n",
        "\n",
        "  next_obs = torch.Tensor(env.reset()).to(device)\n",
        "\n",
        "  # list of episodic returns\n",
        "  ep_return = []\n",
        "\n",
        "  for episode in range(0, num_episodes):\n",
        "    # start the episode\n",
        "    print('Starting episode %d' %episode, end = '\\r')\n",
        "    for step in range(0, step_evaluation):\n",
        "      obs[step] = next_obs\n",
        "\n",
        "      # selecting action with the actor\n",
        "      with torch.no_grad():\n",
        "          action, logprob, _, value = agent_v.get_action_and_value(next_obs)\n",
        "          values[step] = value.flatten()\n",
        "      actions[step] = action\n",
        "      logprobs[step] = logprob\n",
        "\n",
        "      # do the action and getting the rewards\n",
        "      next_obs, reward, done, info = env.step(action.cpu().numpy())\n",
        "      rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
        "      next_obs = torch.tensor(next_obs).to(device)\n",
        "    # getting the return for the episode\n",
        "    ep_return.append(sum(rewards))\n",
        "\n",
        "  # burocracy so that he doesn't complain\n",
        "  ep_return = torch.tensor(ep_return)\n",
        "  ep_mean, ep_std = torch.mean(ep_return).item(), torch.std(ep_return).item()\n",
        "  return ep_mean, ep_std"
      ],
      "metadata": {
        "id": "1HtoB3NlUDcb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_args():\n",
        "    # fmt: off\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--exp-name\", type=str, default= \"TheRealMegaFufi_adventures\",\n",
        "        help=\"the name of this experiment\")\n",
        "    parser.add_argument(\"--gym-id\", type=str, default=\"PIP-v0\",\n",
        "        help=\"the id of the gym environment\")\n",
        "    parser.add_argument(\"--learning-rate\", type=float, default=0.00025,\n",
        "        help=\"the learning rate of the optimizer\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=1,\n",
        "        help=\"seed of the experiment\")\n",
        "    parser.add_argument(\"--total-timesteps\", type=int, default= 50,\n",
        "        help=\"total timesteps of the experiments\")\n",
        "    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n",
        "    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"if toggled, cuda will be enabled by default\")\n",
        "\n",
        "    #W&B setup\n",
        "    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n",
        "    parser.add_argument(\"--wandb-project-name\", type=str, default=\"PIPPA\",\n",
        "        help=\"the wandb's project name\")\n",
        "    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n",
        "        help=\"the entity (team) of wandb's project\")\n",
        "    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"weather to capture videos of the agent performances (check out `videos` folder)\")\n",
        "\n",
        "    # Algorithm specific hyperparameter\n",
        "    parser.add_argument(\"--beta\", type=int, default=0.35,\n",
        "        help=\"weight of continuity cost\")\n",
        "    parser.add_argument(\"--num-envs\", type=int, default=1,\n",
        "        help=\"the number of parallel game environments\")\n",
        "    parser.add_argument(\"--num-steps\", type=int, default=12,\n",
        "        help=\"the number of steps to run in each environment per policy rollout\")\n",
        "    parser.add_argument(\"--anneal-lr\", type=lambda x: bool(strtobool(x)), default= True, nargs=\"?\", const=True,\n",
        "        help=\"Toggle learning rate annealing for policy and value networks\")\n",
        "    parser.add_argument(\"--gae\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"Use GAE for advantage computation\")\n",
        "    parser.add_argument(\"--gamma\", type=float, default=0.99,\n",
        "        help=\"the discount factor gamma\")\n",
        "    parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n",
        "        help=\"the lambda for the general advantage estimation\")\n",
        "    parser.add_argument(\"--num-minibatches\", type=int, default=4,\n",
        "        help=\"the number of mini-batches\")\n",
        "    parser.add_argument(\"--update-epochs\", type=int, default=20,\n",
        "        help=\"the K epochs to update the policy\")\n",
        "    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"Toggles advantages normalization\")\n",
        "    parser.add_argument(\"--clip-coef\", type=float, default=0.2,\n",
        "        help=\"the surrogate clipping coefficient\")\n",
        "    parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n",
        "    parser.add_argument(\"--ent-coef\", type=float, default=0.01,\n",
        "        help=\"coefficient of the entropy\")\n",
        "    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n",
        "        help=\"coefficient of the value function\")\n",
        "    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n",
        "        help=\"the maximum norm for the gradient clipping\")\n",
        "    parser.add_argument(\"--target-kl\", type=float, default=None,\n",
        "        help=\"the target KL divergence threshold\") #should be set to 0.015 if wanna use\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    args.batch_size = int(args.num_envs * args.num_steps)\n",
        "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
        "    # fmt: on\n",
        "    return args"
      ],
      "metadata": {
        "id": "Bz2_1tF6S7P2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = parse_args()\n",
        "run_name = f\"{args.gym_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
        "\n",
        "# env setup\n",
        "envs = vectorize_env(args.gym_id, args.seed, args.capture_video, run_name, args.num_envs, args.beta)\n",
        "# Agent setup\n",
        "agent = Agent(envs, use_sde = True).to(device)\n",
        "#agent.print_summary(envs)\n",
        "optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\n",
        "# initializing things\n",
        "# ALGO Logic: Storage setup\n",
        "obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
        "actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
        "logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "\n",
        "## ------------------------------------- START THE GAME -------------------------------------------\n",
        "# TRY NOT TO MODIFY: start the game\n",
        "global_step = 0\n",
        "start_time = time.time()\n",
        "next_obs = torch.Tensor(envs.reset()).to(device)\n",
        "next_done = torch.zeros(args.num_envs).to(device)\n",
        "num_updates = args.total_timesteps // args.batch_size\n",
        "\n",
        "for update in range(1, num_updates + 1):\n",
        "  #print('Starting update {}'.format(update))\n",
        "  # Annealing the rate if instructed to do so.\n",
        "  optimizer.param_groups[0][\"lr\"] = anneal(args.anneal_lr, update, num_updates, \\\n",
        "                                            args.learning_rate)\n",
        "\n",
        "  for step in range(0, args.num_steps):\n",
        "    # update global steps\n",
        "    global_step += 1 * args.num_envs\n",
        "    #update parameters\n",
        "    obs, actions, logprobs, rewards, dones, values, next_obs, next_done, info \\\n",
        "    = collect_data(envs, obs, actions, logprobs, rewards, dones, values, next_obs,\\\n",
        "                    next_done, agent, step, device)\n",
        "\n",
        "  # general advantages estimation\n",
        "  returns, advantages = GAE(args.gae, args.gae_lambda, args.gamma, agent,\\\n",
        "      values, dones, rewards, next_obs, next_done,\\\n",
        "      args.num_steps, device)\n",
        "  # flatten the batch\n",
        "  b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
        "  b_logprobs = logprobs.reshape(-1)\n",
        "  b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
        "  b_advantages = advantages.reshape(-1)\n",
        "  b_returns = returns.reshape(-1)\n",
        "  b_values = values.reshape(-1)\n",
        "\n",
        "## ------------------------------------- TRAINING LOOP ----------------------------------------------\n",
        "  v_loss, pg_loss, entropy_loss, old_approx_kl, approx_kl, clipfracs,\\\n",
        "  b_values, b_returns = PPO_train_agent(args.batch_size, args.update_epochs, args.minibatch_size, \\\n",
        "                                    args.clip_coef, args.norm_adv, args.clip_vloss,\\\n",
        "                                    args.ent_coef, args.vf_coef, args.max_grad_norm, args.target_kl,\\\n",
        "                                    agent, optimizer, scheduler, False,\\\n",
        "                                    b_obs, b_actions,b_logprobs,\\\n",
        "                                    b_advantages, b_returns, b_values)\n",
        "\n",
        "## --------------------------------- UPDATING AND CLOSING UP -----------------------------------------\n",
        "  y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
        "  var_y = np.var(y_true)\n",
        "  explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "envs.close()"
      ],
      "metadata": {
        "id": "CTuV6Nqf8T8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5736eb1a-def0-48a0-cbd1-34f195d8d1eb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dissennatore\n",
            "Repository Real_Mega_Fufi cloned successfully.\n",
            "Changed directory to Real_Mega_Fufi/Robot/PIPPA.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/vector/vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done environment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:67: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"render.modes\"] is marked as deprecated and will be replaced with `env.metadata[\"render_modes\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment PIP-v0 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:149: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:280: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository Real_Mega_Fufi cloned successfully.\n",
            "Changed directory to Real_Mega_Fufi/Robot/PIPPA.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tywqZ4KqYwaQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}